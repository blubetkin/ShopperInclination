---
title: "Shopper Inclination"
author: "Benjamin Lubetkin"
output:
  html_document:
    df_print: paged
  pdf_document: default
---
##Intro and Data Importing
```{r}
#install.packages("tidyverse", repos = "http://cran.us.r-project.org")
library("tidyverse")
library("dplyr")
library("tidyr")
library("ggplot2")
```
The title of this data set is "Online Shoppers Purchasing Intention Data Set it is from the UCI machine learing repository. Here is the cited source: Sakar, C.O., Polat, S.O., Katircioglu, M. et al. Neural Comput & Applic (2018).

This data set is showing the inclination of online shoppers to buy certain products online when they are new or returning to a new website. It follows over 12,000 people as they buy and not buy. I chose this data set because I, as many others (at least 12,000), do enjoy partaking in online shopping, especially since credit cards are a thing and credit reports are just numbers. First, let us import the data and we can get started!

```{r}
Shoppers <- read_csv("https://raw.githubusercontent.com/blubetkin/ShopperInclination/master/online_shoppers_intention.csv")
```
## Data Summary
I do believe that this data is good, as it covers a wide array of people. For example, since each observation follows one person I feel like each variable can be easily followed as well as interacted with for each person. Another thing I like is how it takes data based off of a whole browsing session rather than just a single website. This can give us a small window into maybe what the person was thinking when they made a purchase, were they shopping around or getting to swiping that sweet plastic? It also has the added benefit of being attached to and used for an academic research paper entitled, "Real-time prediction of online shoppersâ€™ purchasing intention using multilayer perceptron and LSTM recurrent neural networks" by C. Okan Sakar, S. Olcay Polat, Mete Katircioglu, and Yomi Kastro. Thus, I find that it is most definitely on the trustworthy side. It is hard for me to find limitations with it. It does rely on numbers in certain areas where actual words would make more sense (like in the browser variable) and it is a little general, so anyone wanting to find information on certain products being sold or websites being used should look elsewhere. That being said, it is a very good data set!

Now, each column is very strange (at least that is how they appear to be), so let's break down what these variables (columns) mean!

-First, we can actually group the first six columns together! How exciting! And time saving! Each one stands for a certain type of website that was visited during the browsing session and how long they were on the website for. Whether they were Administrative, Informational, or Product Related. The time on the duration variables are in seconds.

-Second, we come to the bounce rate, which is the percentage of people who entered into the site with the product and then left without doing anything related to purchasing.

-Third, there is the exit rate, which is the percentage of page views in the last session out of all pageviews

-Fourth, we find the Page Value, which is the average value of the page visited before a purchase is made

-Fifth, there is the Special Day, which is a numerical value that determines how close to a special day the product's website is visited (Special Day being a holiday) It is calculated by looking at the order and delivery dates of items around the day. The closer to 1 the number is, the more likely the site visitation, and possible order, was made due to a holiday.

-Sixth, the next variables all deal with the month the shopping was made in, the numerical values of the browser that was being used, the operating system that the shopper was using, region the shopper was in, and traffic type that the user belonged to. It also contains the variable visitor type, which details if the shopper was new or returning to the site.

-Seventh, finally, the last two variables are called Weekend and Revenue. These are given boolean statements (True or False) to state whether someone was shopping on a weekend and if they made a purchase or not (in the Revenue column).
## Data Cleaning
Now that we have sorted these out, we can clean the data:

For readability, let's rename the some of the columns,
```{r}
colnames(Shoppers)[colnames(Shoppers) == "Administrative"] <- "Administrative_Website"
colnames(Shoppers)[colnames(Shoppers) == "Informational"] <- "Informational_Website"
colnames(Shoppers)[colnames(Shoppers) == "ProductRelated"] <- "ProductRelated_Website"
colnames(Shoppers)[colnames(Shoppers) == "BounceRates"] <- "Percentage_that_Left_Website_Immediately"
colnames(Shoppers)[colnames(Shoppers) == "ExitRates"] <- "Percentage_of_Page_Views_in_Last_Session"
colnames(Shoppers)[colnames(Shoppers) == "Administrative"] <- "Administrative_Website"
colnames(Shoppers)[colnames(Shoppers) == "Revenue"] <- "Purchase"
colnames(Shoppers)[colnames(Shoppers) == "Region"] <- "Shoppers_Region"
colnames(Shoppers)[colnames(Shoppers) == "SpecialDay"] <- "Holiday"
```
Now, this data set seems very big. It would be a good idea to separate some columns into some other tables. First, let's create a table that takes all of the data about the websites that were visited before a purchase was made and put them into their own table. This will allow us to have a quick reference for any information on the websites. 

```{r}
Websites <- tibble(Administrative_Website = Shoppers$Administrative_Website, Administrative_Duration = Shoppers$Administrative_Duration, Informational_Website = Shoppers$Informational_Website, Informational_Duration = Shoppers$Informational_Duration, ProductRelated = Shoppers$ProductRelated_Website, ProductRelated_Duration = Shoppers$ProductRelated_Duration, Purchase = Shoppers$Purchase)
```

Next, we can create a table dedicated to how long shoppers spent on the site that they made their purchase on. We are ceating this seperate table because it will allow us to easily access information that is directly related to how the person shopped. This table is what we will look at if a purchase was made on impulse or not.

```{r}
Shopping <- tibble(Percentage_that_Left_Website_Immediately = Shoppers$Percentage_that_Left_Website_Immediately, Percentage_of_Page_Views_in_Last_Session = Shoppers$Percentage_of_Page_Views_in_Last_Session, PageValues = Shoppers$PageValues, Holiday = Shoppers$Holiday, Month = Shoppers$Month, Purchase = Shoppers$Purchase)
```

Finally, we will make a final table that collects all the data on the shoppers, their computers, what browsers they are using, region they're in, etc. This will make it easy for us to identify things we want to know about the shoppers themselves.

```{r}
TheShoppers <- tibble(OperatingSystems = Shoppers$OperatingSystems, Browser = Shoppers$Browser, Shoppers_Region = Shoppers$Shoppers_Region, TrafficType = Shoppers$TrafficType, VisitorType = Shoppers$VisitorType, Weekend = Shoppers$Weekend, Purchase = Shoppers$Purchase)
```
## Visualization
The next thing I will do is take some of the data and visualize it. The first thing we will visualize is how many shoppers that were online in this given time actually made a purchase. To represent this we will use a simple bar chart:

```{r}
ggplot(TheShoppers, aes(x = Purchase)) + geom_bar()
```

Wowzers! Look at how many shoppers are responsible! They didn't buy anything! This will surely be seen in the data later on. We can use it in the other questions about purchases made. Now, let's make a bar graph about how many returning shoppers there were versus how many were new:

```{r}
ggplot(TheShoppers, aes(x = VisitorType)) + geom_bar()
```
From this we can see that most of the shoppers, and possible buyers, are actually returning, thus we can think that they might have possibly been planning on this purchase and not just buying impulsively. This information can be used in any questions related to just new visitors or just returning visitors. Next, let's make a graph of the amount of shopping being done on the weekend:

```{r}
ggplot(TheShoppers, aes(x = Weekend)) + geom_bar()
```
This shows that most of the shoppers did their shopping on the weekdays. This could be from people getting done with work and relaxing by shopping online. If that is the case, then the weekdays would be the best time for ads to be targeted to users if an advertising agency was looking at this data.

Now, the final graph I would like to make is a bar graph related to how many people were visiting sites close to the holidays:
```{r}
ggplot(Shopping, aes(x = Holiday)) + geom_bar()
```
From this, we can see that most of the possible shopping was done on days not related to holidays. This is important to see because it doesn't just take into account shoppers purchasing, but also shoppers not purchasing, thus we get a more well-rounded view of when a majority of the shopping is taking place. This is our first visualization with this, but I do think we will be breaking it up in the future and looking at different collections related to buyers and non-buyers and these holidays.

## Final Points and Initial Research Questions
Now that we have done the intro to my data set and done some cleaning, I believe it is time to look at what I would like to find based on this data set. My questions I would like to answer are:

-To determine impulse purchases, how many new visitors made a purchase? This information could be used to model advertising and whether it is directed more towards new or returning customers.


-Is there a correlation between the page values and a purchase? As in, did research go into this purchase? This is different from checking to see impulse buying, because it looks at whether research into different amounts of websites caused that person to decide against making a purchase or going through with one.

-What was the average time spent on all the websites before a purchase was made or not made? This will allow us to determine a frame of mind that a shopper could be "in the shopping mode" before they exit and realize they don't want to make the purchase.

I am very excited to get further into this data and see how much information can be learned from this data set!